{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpWTHYyioLKY",
        "outputId": "b8956183-f028-4816-86bc-dc2ff8d800aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.39.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio pandas numpy matplotlib seaborn plotly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "1C0bihEWn0hc",
        "outputId": "172dbee1-e472-479c-d45c-23153b1695fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9d8cd7cfde7f4f565f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9d8cd7cfde7f4f565f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced data preprocessing functions\n",
        "def convert_into_bytes(column_name):\n",
        "    if isinstance(column_name, str):\n",
        "        if 'k' in column_name.lower():\n",
        "            return float(column_name.lower().replace(\"k\", \"\")) * 1024\n",
        "        elif 'm' in column_name.lower():\n",
        "            return float(column_name.lower().replace(\"m\", \"\")) * 1024 * 1024\n",
        "        elif 'g' in column_name.lower():\n",
        "            return float(column_name.lower().replace(\"g\", \"\")) * 1024 * 1024 * 1024\n",
        "        elif 'varies with device' in column_name.lower():\n",
        "            return np.nan\n",
        "        else:\n",
        "            try:\n",
        "                return float(column_name)\n",
        "            except:\n",
        "                return np.nan\n",
        "    return column_name\n",
        "\n",
        "def installs_cleaner(install):\n",
        "    if isinstance(install, str):\n",
        "        install = install.replace(\"+\", \"\").replace(\",\", \"\")\n",
        "        try:\n",
        "            return int(install)\n",
        "        except:\n",
        "            return 0\n",
        "    return install if pd.notna(install) else 0\n",
        "\n",
        "def adjust_price(price):\n",
        "    if isinstance(price, str):\n",
        "        if '$' in price:\n",
        "            try:\n",
        "                return float(price.replace(\"$\", \"\"))\n",
        "            except:\n",
        "                return 0.0\n",
        "    return price if pd.notna(price) else 0.0\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    \"\"\"Create advanced features for better prediction\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Review-to-Install ratio (engagement metric)\n",
        "    if 'Reviews' in df.columns and 'Installs' in df.columns:\n",
        "        df_enhanced['Review_Install_Ratio'] = np.where(\n",
        "            df_enhanced['Installs'] > 0,\n",
        "            df_enhanced['Reviews'] / (df_enhanced['Installs'] + 1),\n",
        "            0\n",
        "        )\n",
        "\n",
        "    # Log transformations for skewed features\n",
        "    if 'Reviews' in df.columns:\n",
        "        df_enhanced['Log_Reviews'] = np.log1p(df_enhanced['Reviews'])\n",
        "    if 'Installs' in df.columns:\n",
        "        df_enhanced['Log_Installs'] = np.log1p(df_enhanced['Installs'])\n",
        "    if 'Size_MB' in df.columns:\n",
        "        df_enhanced['Log_Size'] = np.log1p(df_enhanced['Size_MB'].fillna(0))\n",
        "\n",
        "    # Price categories\n",
        "    if 'Price' in df.columns:\n",
        "        df_enhanced['Price_Category'] = pd.cut(\n",
        "            df_enhanced['Price'],\n",
        "            bins=[-0.1, 0, 1, 5, 20, float('inf')],\n",
        "            labels=['Free', 'Cheap', 'Moderate', 'Expensive', 'Premium']\n",
        "        )\n",
        "\n",
        "    # Category popularity score\n",
        "    if 'Category' in df.columns:\n",
        "        category_counts = df_enhanced['Category'].value_counts()\n",
        "        df_enhanced['Category_Popularity'] = df_enhanced['Category'].map(category_counts)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Enhanced comprehensive data preprocessing pipeline\"\"\"\n",
        "    cleaning_log = []\n",
        "\n",
        "    try:\n",
        "        original_rows = len(df)\n",
        "        cleaning_log.append(f\"üìä Original dataset: {original_rows:,} rows\")\n",
        "\n",
        "        # Remove problematic rows\n",
        "        if 'Category' in df.columns:\n",
        "            problematic_rows = df[df['Category'].astype(str).str.contains(r'^\\d+\\.\\d+$', na=False)].index\n",
        "            if len(problematic_rows) > 0:\n",
        "                df = df.drop(problematic_rows, errors='ignore')\n",
        "                cleaning_log.append(f\"üóëÔ∏è Removed {len(problematic_rows)} problematic category rows\")\n",
        "\n",
        "        # Convert Reviews to integer\n",
        "        if 'Reviews' in df.columns:\n",
        "            original_nulls = df['Reviews'].isnull().sum()\n",
        "            df['Reviews'] = pd.to_numeric(df['Reviews'], errors='coerce').fillna(0).astype(int)\n",
        "            cleaning_log.append(f\"üî¢ Reviews column: Fixed {original_nulls} null values\")\n",
        "\n",
        "        # Process Size column with enhanced conversion\n",
        "        if 'Size' in df.columns:\n",
        "            original_nulls = df['Size'].isnull().sum()\n",
        "            df['Size'] = df['Size'].apply(convert_into_bytes)\n",
        "            df['Size_MB'] = df['Size'] / (1024 * 1024)\n",
        "            # Fill missing sizes with median by category\n",
        "            if 'Category' in df.columns:\n",
        "                df['Size_MB'] = df.groupby('Category')['Size_MB'].transform(\n",
        "                    lambda x: x.fillna(x.median())\n",
        "                )\n",
        "            df['Size_MB'] = df['Size_MB'].fillna(df['Size_MB'].median())\n",
        "            cleaning_log.append(f\"üì± Size column: Enhanced conversion, {original_nulls} null values handled\")\n",
        "\n",
        "        # Enhanced Installs processing\n",
        "        if 'Installs' in df.columns:\n",
        "            original_nulls = df['Installs'].isnull().sum()\n",
        "            df['Installs'] = df['Installs'].apply(installs_cleaner)\n",
        "            df['Installs'] = pd.to_numeric(df['Installs'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "            # Create more granular Install categories\n",
        "            bins = [-1, 0, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 10000000000]\n",
        "            labels = ['No installs', 'Very low', 'Low', 'Moderate', 'Good', 'High', 'Very High', 'Excellent', 'Top Tier', 'Viral']\n",
        "            df['Installs_category'] = pd.cut(df['Installs'], bins=bins, labels=labels)\n",
        "            cleaning_log.append(f\"üìà Installs column: Enhanced categories, {original_nulls} null values handled\")\n",
        "\n",
        "        # Enhanced Price processing\n",
        "        if 'Price' in df.columns:\n",
        "            original_nulls = df['Price'].isnull().sum()\n",
        "            df['Price'] = df['Price'].apply(adjust_price)\n",
        "            df['Price'] = pd.to_numeric(df['Price'], errors='coerce').fillna(0)\n",
        "            cleaning_log.append(f\"üí∞ Price column: Cleaned format, {original_nulls} null values handled\")\n",
        "\n",
        "        # Smart Rating handling with multiple strategies\n",
        "        if 'Rating' in df.columns:\n",
        "            original_rating_nulls = df['Rating'].isnull().sum()\n",
        "\n",
        "            # Strategy 1: Fill by category and install level\n",
        "            if 'Category' in df.columns and 'Installs_category' in df.columns:\n",
        "                for cat in df['Category'].unique():\n",
        "                    for inst_cat in df['Installs_category'].unique():\n",
        "                        if pd.notna(cat) and pd.notna(inst_cat):\n",
        "                            mask = (df['Category'] == cat) & (df['Installs_category'] == inst_cat) & df['Rating'].isnull()\n",
        "                            if mask.sum() > 0:\n",
        "                                mean_rating = df[(df['Category'] == cat) & (df['Installs_category'] == inst_cat) & df['Rating'].notna()]['Rating'].mean()\n",
        "                                if pd.notna(mean_rating):\n",
        "                                    df.loc[mask, 'Rating'] = mean_rating\n",
        "\n",
        "            # Strategy 2: Fill remaining by category only\n",
        "            remaining_nulls = df['Rating'].isnull()\n",
        "            if remaining_nulls.sum() > 0 and 'Category' in df.columns:\n",
        "                category_means = df.groupby('Category')['Rating'].mean()\n",
        "                for cat in df['Category'].unique():\n",
        "                    if pd.notna(cat):\n",
        "                        mask = (df['Category'] == cat) & df['Rating'].isnull()\n",
        "                        if mask.sum() > 0:\n",
        "                            df.loc[mask, 'Rating'] = category_means.get(cat, 4.0)\n",
        "\n",
        "            # Strategy 3: Fill any remaining with global median\n",
        "            df['Rating'] = df['Rating'].fillna(df['Rating'].median())\n",
        "\n",
        "            final_rating_nulls = df['Rating'].isnull().sum()\n",
        "            fixed_ratings = original_rating_nulls - final_rating_nulls\n",
        "            cleaning_log.append(f\"‚≠ê Rating column: Smart filling fixed {fixed_ratings} missing ratings\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        original_rows_before_dedup = len(df)\n",
        "        df = df.drop_duplicates()\n",
        "        duplicates_removed = original_rows_before_dedup - len(df)\n",
        "        if duplicates_removed > 0:\n",
        "            cleaning_log.append(f\"üîÑ Removed {duplicates_removed} duplicate rows\")\n",
        "\n",
        "        # Create advanced features\n",
        "        df = create_advanced_features(df)\n",
        "        cleaning_log.append(f\"üöÄ Created advanced features for better predictions\")\n",
        "\n",
        "        final_rows = len(df)\n",
        "        cleaning_log.append(f\"‚úÖ Final dataset: {final_rows:,} rows ({final_rows - original_rows:+,} change)\")\n",
        "\n",
        "        return df, cleaning_log\n",
        "    except Exception as e:\n",
        "        cleaning_log.append(f\"‚ùå Error during preprocessing: {str(e)}\")\n",
        "        return df, cleaning_log\n",
        "\n",
        "def preprocess_reviews_data(df_reviews):\n",
        "    \"\"\"Preprocess user reviews data\"\"\"\n",
        "    if df_reviews is None or df_reviews.empty:\n",
        "        return df_reviews\n",
        "\n",
        "    try:\n",
        "        # Basic cleaning for reviews dataset\n",
        "        df_reviews = df_reviews.dropna(subset=['App'])\n",
        "        df_reviews = df_reviews.drop_duplicates()\n",
        "        return df_reviews\n",
        "    except Exception as e:\n",
        "        print(f\"Reviews preprocessing error: {e}\")\n",
        "        return df_reviews\n",
        "\n",
        "# Enhanced visualization functions\n",
        "def create_category_distribution(df):\n",
        "    \"\"\"Create interactive category distribution plot\"\"\"\n",
        "    if df is None or df.empty or 'Category' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        category_counts = df['Category'].value_counts().head(15)\n",
        "\n",
        "        fig = px.bar(\n",
        "            x=category_counts.index,\n",
        "            y=category_counts.values,\n",
        "            title=\"Top 15 App Categories by Count\",\n",
        "            labels={'x': 'Category', 'y': 'Number of Apps'},\n",
        "            color=category_counts.values,\n",
        "            color_continuous_scale='viridis',\n",
        "            text=category_counts.values\n",
        "        )\n",
        "        fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
        "        fig.update_layout(xaxis_tickangle=-45, height=600)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Category distribution error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_rating_distribution(df):\n",
        "    \"\"\"Create rating distribution with category breakdown\"\"\"\n",
        "    if df is None or df.empty or 'Rating' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        fig = px.histogram(\n",
        "            df,\n",
        "            x='Rating',\n",
        "            color='Installs_category' if 'Installs_category' in df.columns else None,\n",
        "            title=\"Rating Distribution by Install Category\",\n",
        "            nbins=20,\n",
        "            barmode='overlay',\n",
        "            opacity=0.7\n",
        "        )\n",
        "        fig.update_layout(height=500)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Rating distribution error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_installs_vs_reviews(df):\n",
        "    \"\"\"Create scatter plot of installs vs reviews\"\"\"\n",
        "    if df is None or df.empty or 'Reviews' not in df.columns or 'Installs' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Sample data for better performance\n",
        "        sample_df = df.sample(min(5000, len(df)))\n",
        "\n",
        "        fig = px.scatter(\n",
        "            sample_df,\n",
        "            x=np.log10(sample_df['Reviews'] + 1),\n",
        "            y=np.log10(sample_df['Installs'] + 1),\n",
        "            color='Rating' if 'Rating' in df.columns else None,\n",
        "            size='Size_MB' if 'Size_MB' in df.columns else None,\n",
        "            title=\"Relationship between Reviews and Installs (Log Scale)\",\n",
        "            labels={'x': 'Log10(Reviews + 1)', 'y': 'Log10(Installs + 1)'},\n",
        "            hover_data=['App', 'Category', 'Rating'] if all(col in df.columns for col in ['App', 'Category', 'Rating']) else None\n",
        "        )\n",
        "        fig.update_layout(height=600)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Scatter plot error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_category_metrics(df):\n",
        "    \"\"\"Create comprehensive category metrics\"\"\"\n",
        "    if df is None or df.empty or 'Category' not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        metrics_dict = {}\n",
        "\n",
        "        if 'Rating' in df.columns:\n",
        "            metrics_dict['Avg_Rating'] = df.groupby('Category')['Rating'].mean()\n",
        "        if 'Reviews' in df.columns:\n",
        "            metrics_dict['Total_Reviews'] = df.groupby('Category')['Reviews'].sum()\n",
        "            metrics_dict['Avg_Reviews'] = df.groupby('Category')['Reviews'].mean()\n",
        "        if 'Installs' in df.columns:\n",
        "            metrics_dict['Total_Installs'] = df.groupby('Category')['Installs'].sum()\n",
        "            metrics_dict['Avg_Installs'] = df.groupby('Category')['Installs'].mean()\n",
        "        if 'Price' in df.columns:\n",
        "            metrics_dict['Avg_Price'] = df.groupby('Category')['Price'].mean()\n",
        "\n",
        "        metrics_dict['App_Count'] = df.groupby('Category').size()\n",
        "\n",
        "        metrics = pd.DataFrame(metrics_dict).round(2)\n",
        "\n",
        "        # Sort by total installs if available, otherwise by app count\n",
        "        if 'Total_Installs' in metrics.columns:\n",
        "            metrics = metrics.sort_values('Total_Installs', ascending=False).head(15)\n",
        "        else:\n",
        "            metrics = metrics.sort_values('App_Count', ascending=False).head(15)\n",
        "\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(f\"Category metrics error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def create_price_analysis(df):\n",
        "    \"\"\"Enhanced price analysis\"\"\"\n",
        "    if df is None or df.empty or 'Price' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df_copy = df.copy()\n",
        "        df_copy['App_Type'] = df_copy['Price'].apply(lambda x: 'Free' if pd.isna(x) or x == 0 else 'Paid')\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            subplot_titles=('Free vs Paid Apps', 'Price Distribution (Paid Apps)'),\n",
        "            specs=[[{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
        "        )\n",
        "\n",
        "        # Free vs Paid\n",
        "        type_counts = df_copy['App_Type'].value_counts()\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=type_counts.index, y=type_counts.values, name=\"App Type\"),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Price distribution for paid apps\n",
        "        paid_apps = df_copy[df_copy['Price'] > 0]['Price']\n",
        "        if len(paid_apps) > 0:\n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=paid_apps, nbinsx=20, name=\"Price Distribution\"),\n",
        "                row=1, col=2\n",
        "            )\n",
        "\n",
        "        fig.update_layout(height=500, showlegend=False, title_text=\"Enhanced Price Analysis\")\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Price analysis error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_correlation_heatmap(df):\n",
        "    \"\"\"Create enhanced correlation heatmap\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        numeric_cols = ['Rating', 'Reviews', 'Installs', 'Price', 'Size_MB', 'Log_Reviews', 'Log_Installs', 'Review_Install_Ratio']\n",
        "        available_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "        if len(available_cols) < 2:\n",
        "            return None\n",
        "\n",
        "        correlation_matrix = df[available_cols].corr()\n",
        "\n",
        "        fig = px.imshow(\n",
        "            correlation_matrix,\n",
        "            text_auto=True,\n",
        "            aspect=\"auto\",\n",
        "            title=\"Enhanced Correlation Matrix of Features\",\n",
        "            color_continuous_scale='RdBu',\n",
        "            zmin=-1, zmax=1\n",
        "        )\n",
        "        fig.update_layout(height=600)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Correlation heatmap error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_sentiment_analysis(df_reviews):\n",
        "    \"\"\"Analyze sentiment from reviews data\"\"\"\n",
        "    if df_reviews is None or df_reviews.empty or 'Sentiment' not in df_reviews.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        sentiment_counts = df_reviews['Sentiment'].value_counts()\n",
        "\n",
        "        fig = px.pie(\n",
        "            values=sentiment_counts.values,\n",
        "            names=sentiment_counts.index,\n",
        "            title=\"Sentiment Distribution of User Reviews\",\n",
        "            color_discrete_sequence=px.colors.qualitative.Set3\n",
        "        )\n",
        "        fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "        fig.update_layout(height=500)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Sentiment analysis error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_size_distribution(df):\n",
        "    \"\"\"Create app size distribution\"\"\"\n",
        "    if df is None or df.empty or 'Size_MB' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        size_data = df['Size_MB'].dropna()\n",
        "        size_data = size_data[size_data <= size_data.quantile(0.95)]  # Remove extreme outliers\n",
        "\n",
        "        fig = px.histogram(\n",
        "            x=size_data,\n",
        "            nbins=40,\n",
        "            title=\"App Size Distribution (MB) - 95th Percentile\",\n",
        "            labels={'x': 'Size (MB)', 'y': 'Count'},\n",
        "            marginal=\"box\"  # Add box plot\n",
        "        )\n",
        "        fig.update_layout(height=500)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Size distribution error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_installs_pie(df):\n",
        "    \"\"\"Create installs category pie chart\"\"\"\n",
        "    if df is None or df.empty or 'Installs_category' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        installs_counts = df['Installs_category'].value_counts()\n",
        "\n",
        "        fig = px.pie(\n",
        "            values=installs_counts.values,\n",
        "            names=installs_counts.index,\n",
        "            title=\"Installs Category Distribution\",\n",
        "            color_discrete_sequence=px.colors.qualitative.Pastel\n",
        "        )\n",
        "        fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "        fig.update_layout(height=500)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Installs pie error: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_missing_values_plot(df):\n",
        "    \"\"\"Create missing values visualization\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        missing_pct = (df.isnull().mean() * 100).sort_values(ascending=False)\n",
        "        missing_pct = missing_pct[missing_pct > 0]\n",
        "\n",
        "        if missing_pct.empty:\n",
        "            fig = go.Figure()\n",
        "            fig.add_annotation(\n",
        "                text=\"No missing values found! ‚úÖ\",\n",
        "                xref=\"paper\", yref=\"paper\",\n",
        "                x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "                showarrow=False,\n",
        "                font=dict(size=20, color=\"green\")\n",
        "            )\n",
        "            fig.update_layout(title=\"Missing Values Analysis\", height=400)\n",
        "            return fig\n",
        "\n",
        "        fig = px.bar(\n",
        "            x=missing_pct.index,\n",
        "            y=missing_pct.values,\n",
        "            title=\"Missing Values Percentage by Column\",\n",
        "            labels={'x': 'Column', 'y': 'Percentage Missing'},\n",
        "            color=missing_pct.values,\n",
        "            color_continuous_scale='Reds'\n",
        "        )\n",
        "        fig.update_layout(height=400, xaxis_tickangle=-45)\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Missing values plot error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_duplicates_info(df):\n",
        "    \"\"\"Get duplicates information\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return \"No data available\"\n",
        "\n",
        "    try:\n",
        "        duplicates = df.duplicated().sum()\n",
        "        total_rows = len(df)\n",
        "        duplicate_pct = (duplicates / total_rows) * 100\n",
        "\n",
        "        return f\"\"\"\n",
        "üîÅ **Duplicate Analysis:**\n",
        "‚Ä¢ Total duplicates: **{duplicates:,}**\n",
        "‚Ä¢ Total rows: **{total_rows:,}**\n",
        "‚Ä¢ Duplicate percentage: **{duplicate_pct:.2f}%**\n",
        "        \"\"\"\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing duplicates: {e}\"\n",
        "\n",
        "def get_safe_info_string(df_processed):\n",
        "    \"\"\"Generate safe info string\"\"\"\n",
        "    if df_processed is None or df_processed.empty:\n",
        "        return \"‚ùå No data processed. Please check your CSV file format.\"\n",
        "\n",
        "    try:\n",
        "        total_apps = len(df_processed)\n",
        "        categories = df_processed['Category'].nunique() if 'Category' in df_processed.columns else 0\n",
        "        unique_apps = df_processed['App'].nunique() if 'App' in df_processed.columns else 0\n",
        "\n",
        "        # Safe rating calculation\n",
        "        if 'Rating' in df_processed.columns and not df_processed['Rating'].isnull().all():\n",
        "            avg_rating = f\"{df_processed['Rating'].mean():.2f}\"\n",
        "            rating_std = f\"{df_processed['Rating'].std():.2f}\"\n",
        "        else:\n",
        "            avg_rating = \"N/A\"\n",
        "            rating_std = \"N/A\"\n",
        "\n",
        "        # Safe reviews calculation\n",
        "        if 'Reviews' in df_processed.columns:\n",
        "            total_reviews = f\"{df_processed['Reviews'].sum():,}\"\n",
        "            avg_reviews = f\"{df_processed['Reviews'].mean():.0f}\"\n",
        "        else:\n",
        "            total_reviews = \"N/A\"\n",
        "            avg_reviews = \"N/A\"\n",
        "\n",
        "        # Safe price calculations\n",
        "        if 'Price' in df_processed.columns:\n",
        "            free_apps = f\"{(df_processed['Price'] == 0).sum():,}\"\n",
        "            paid_apps = f\"{(df_processed['Price'] > 0).sum():,}\"\n",
        "            avg_price = f\"${df_processed[df_processed['Price'] > 0]['Price'].mean():.2f}\" if any(df_processed['Price'] > 0) else \"N/A\"\n",
        "        else:\n",
        "            free_apps = \"N/A\"\n",
        "            paid_apps = \"N/A\"\n",
        "            avg_price = \"N/A\"\n",
        "\n",
        "        info_text = f\"\"\"üìä **Enhanced Apps Dataset Overview:**\n",
        "‚Ä¢ Total Apps: {total_apps:,}\n",
        "‚Ä¢ Categories: {categories}\n",
        "‚Ä¢ Unique Apps: {unique_apps:,}\n",
        "‚Ä¢ Average Rating: {avg_rating} (¬±{rating_std})\n",
        "‚Ä¢ Total Reviews: {total_reviews}\n",
        "‚Ä¢ Avg Reviews per App: {avg_reviews}\n",
        "‚Ä¢ Free Apps: {free_apps}\n",
        "‚Ä¢ Paid Apps: {paid_apps}\n",
        "‚Ä¢ Avg Price (Paid): {avg_price}\n",
        "\n",
        "‚úÖ **Data successfully enhanced and ready for advanced analysis!**\"\"\"\n",
        "\n",
        "        return info_text\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating info: {str(e)}\"\n",
        "\n",
        "def get_safe_preview(df_processed):\n",
        "    \"\"\"Generate safe data preview\"\"\"\n",
        "    if df_processed is None or df_processed.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        preview_cols = ['App', 'Category', 'Rating', 'Reviews', 'Installs', 'Price', 'Size_MB']\n",
        "        available_cols = [col for col in preview_cols if col in df_processed.columns]\n",
        "\n",
        "        if not available_cols:\n",
        "            available_cols = list(df_processed.columns)[:8]\n",
        "\n",
        "        return df_processed[available_cols].head(10)\n",
        "    except Exception as e:\n",
        "        print(f\"Preview error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# FIXED: Category choices function\n",
        "def get_category_choices(df_processed):\n",
        "    \"\"\"Get category choices safely with proper sorting - COMPLETELY FIXED\"\"\"\n",
        "    try:\n",
        "        if df_processed is None or df_processed.empty:\n",
        "            return [\"All\"]\n",
        "\n",
        "        if 'Category' not in df_processed.columns:\n",
        "            return [\"All\"]\n",
        "\n",
        "        # Get unique categories, remove NaN/empty values\n",
        "        unique_categories = df_processed['Category'].dropna().unique()\n",
        "        unique_categories = [str(cat).strip() for cat in unique_categories\n",
        "                           if str(cat).strip() not in ['', 'nan', 'None', 'NaN']]\n",
        "\n",
        "        # Sort categories alphabetically\n",
        "        unique_categories = sorted(list(set(unique_categories)))\n",
        "\n",
        "        # Return with \"All\" as first option\n",
        "        return [\"All\"] + unique_categories\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Category choices error: {e}\")\n",
        "        return [\"All\"]\n",
        "\n",
        "# FIXED: Enhanced Predictive Modeling Functions\n",
        "def prepare_enhanced_features_for_modeling(df, selected_features):\n",
        "    \"\"\"Prepare enhanced features for machine learning - FIXED\"\"\"\n",
        "    if df is None or df.empty or 'Rating' not in df.columns:\n",
        "        return None, None, None, None, \"‚ùå No data or Rating column not found\"\n",
        "\n",
        "    try:\n",
        "        # Remove rows with missing ratings\n",
        "        model_df = df.dropna(subset=['Rating']).copy()\n",
        "\n",
        "        if len(model_df) < 100:\n",
        "            return None, None, None, None, \"‚ùå Not enough data for modeling (need at least 100 samples)\"\n",
        "\n",
        "        # Prepare feature matrix\n",
        "        X_data = []\n",
        "        feature_names = []\n",
        "\n",
        "        # Create label encoders dictionary\n",
        "        label_encoders = {}\n",
        "\n",
        "        # Add selected features\n",
        "        for feature in selected_features:\n",
        "            if feature == 'Reviews' and 'Reviews' in model_df.columns:\n",
        "                feature_data = model_df['Reviews'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Reviews')\n",
        "\n",
        "            elif feature == 'Log_Reviews' and 'Log_Reviews' in model_df.columns:\n",
        "                feature_data = model_df['Log_Reviews'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Log_Reviews')\n",
        "\n",
        "            elif feature == 'Installs' and 'Installs' in model_df.columns:\n",
        "                feature_data = model_df['Installs'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Installs')\n",
        "\n",
        "            elif feature == 'Log_Installs' and 'Log_Installs' in model_df.columns:\n",
        "                feature_data = model_df['Log_Installs'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Log_Installs')\n",
        "\n",
        "            elif feature == 'Size_MB' and 'Size_MB' in model_df.columns:\n",
        "                feature_data = model_df['Size_MB'].fillna(model_df['Size_MB'].median()).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Size_MB')\n",
        "\n",
        "            elif feature == 'Price' and 'Price' in model_df.columns:\n",
        "                feature_data = model_df['Price'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Price')\n",
        "\n",
        "            elif feature == 'Review_Install_Ratio' and 'Review_Install_Ratio' in model_df.columns:\n",
        "                feature_data = model_df['Review_Install_Ratio'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Review_Install_Ratio')\n",
        "\n",
        "            elif feature == 'Category' and 'Category' in model_df.columns:\n",
        "                le = LabelEncoder()\n",
        "                category_data = model_df['Category'].fillna('Unknown')\n",
        "                category_encoded = le.fit_transform(category_data)\n",
        "                X_data.append(category_encoded)\n",
        "                feature_names.append('Category')\n",
        "                label_encoders['Category'] = le\n",
        "\n",
        "            elif feature == 'Category_Popularity' and 'Category_Popularity' in model_df.columns:\n",
        "                feature_data = model_df['Category_Popularity'].fillna(0).values\n",
        "                X_data.append(feature_data)\n",
        "                feature_names.append('Category_Popularity')\n",
        "\n",
        "        if not X_data:\n",
        "            return None, None, None, None, \"‚ùå No valid features selected or available in data\"\n",
        "\n",
        "        X = np.column_stack(X_data)\n",
        "        y = model_df['Rating'].values\n",
        "\n",
        "        # Remove any remaining NaN or infinite values\n",
        "        valid_mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
        "        X = X[valid_mask]\n",
        "        y = y[valid_mask]\n",
        "\n",
        "        if len(X) < 100:\n",
        "            return None, None, None, None, \"‚ùå Not enough valid data after cleaning\"\n",
        "\n",
        "        return X, y, feature_names, label_encoders, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, None, None, f\"‚ùå Error preparing features: {str(e)}\"\n",
        "\n",
        "# FIXED: Model training function\n",
        "def train_enhanced_rating_prediction_model(df, selected_features):\n",
        "    \"\"\"Train enhanced machine learning model with multiple algorithms - COMPLETELY FIXED\"\"\"\n",
        "    if not selected_features:\n",
        "        return \"‚ùå Please select at least one feature for training\", None, None, None, None\n",
        "\n",
        "    X, y, feature_names, label_encoders, error = prepare_enhanced_features_for_modeling(df, selected_features)\n",
        "\n",
        "    if error:\n",
        "        return error, None, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Scale features using RobustScaler (better for outliers)\n",
        "        scaler = RobustScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train multiple models\n",
        "        models = {\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6),\n",
        "            'Linear Regression': LinearRegression()\n",
        "        }\n",
        "\n",
        "        best_model = None\n",
        "        best_score = -np.inf\n",
        "        best_model_name = \"\"\n",
        "\n",
        "        model_results = {}\n",
        "\n",
        "        for name, model in models.items():\n",
        "            # Train model\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            # Cross-validation score\n",
        "            try:\n",
        "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='r2')\n",
        "                cv_mean = cv_scores.mean()\n",
        "            except:\n",
        "                cv_mean = r2  # Fallback to test R2\n",
        "\n",
        "            model_results[name] = {\n",
        "                'model': model,\n",
        "                'r2': r2,\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'cv_score': cv_mean\n",
        "            }\n",
        "\n",
        "            # Track best model\n",
        "            if r2 > best_score:\n",
        "                best_score = r2\n",
        "                best_model = model\n",
        "                best_model_name = name\n",
        "\n",
        "        # Prepare results\n",
        "        results_text = f\"\"\"ü§ñ **Enhanced Model Training Results:**\n",
        "\n",
        "üìä **Dataset Info:**\n",
        "‚Ä¢ Training samples: {len(X_train):,}\n",
        "‚Ä¢ Testing samples: {len(X_test):,}\n",
        "‚Ä¢ Features used: {', '.join(feature_names)}\n",
        "\n",
        "üèÜ **Best Model: {best_model_name}**\n",
        "\n",
        "üéØ **Model Comparison:**\"\"\"\n",
        "\n",
        "        for name, results in model_results.items():\n",
        "            emoji = \"ü•á\" if name == best_model_name else \"ü•à\" if results['r2'] == sorted([r['r2'] for r in model_results.values()], reverse=True)[1] else \"ü•â\"\n",
        "            results_text += f\"\"\"\n",
        "{emoji} **{name}:**\n",
        "   ‚Ä¢ R¬≤ Score: {results['r2']:.4f} ({results['r2']*100:.2f}% variance explained)\n",
        "   ‚Ä¢ Cross-Val R¬≤: {results['cv_score']:.4f}\n",
        "   ‚Ä¢ RMSE: {results['rmse']:.4f}\n",
        "   ‚Ä¢ MAE: {results['mae']:.4f}\"\"\"\n",
        "\n",
        "        results_text += f\"\"\"\n",
        "\n",
        "üìà **Best Model Interpretation:**\n",
        "‚Ä¢ Explains {best_score*100:.2f}% of rating variance\n",
        "‚Ä¢ Average prediction error: ¬±{model_results[best_model_name]['mae']:.2f} stars\n",
        "‚Ä¢ Cross-validation stability: {model_results[best_model_name]['cv_score']:.3f}\n",
        "{\"‚Ä¢ üéâ Excellent model performance!\" if best_score > 0.6 else \"‚Ä¢ üéä Good model performance!\" if best_score > 0.4 else \"‚Ä¢ ‚ö†Ô∏è Model needs improvement - try more features\"}\n",
        "\"\"\"\n",
        "\n",
        "        # Feature importance (for tree-based models)\n",
        "        importance_fig = None\n",
        "        if hasattr(best_model, 'feature_importances_'):\n",
        "            importance_fig = create_enhanced_feature_importance_plot(best_model, feature_names)\n",
        "\n",
        "        # FIXED: Return the actual model object, not results\n",
        "        return results_text, best_model, scaler, importance_fig, label_encoders\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error training model: {str(e)}\", None, None, None, None\n",
        "\n",
        "def create_enhanced_feature_importance_plot(model, feature_names):\n",
        "    \"\"\"Create enhanced feature importance visualization\"\"\"\n",
        "    try:\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Sort by importance\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        sorted_features = [feature_names[i] for i in indices]\n",
        "        sorted_importances = importances[indices]\n",
        "\n",
        "        fig = px.bar(\n",
        "            x=sorted_importances,\n",
        "            y=sorted_features,\n",
        "            orientation='h',\n",
        "            title=\"Feature Importance for Rating Prediction\",\n",
        "            labels={'x': 'Importance', 'y': 'Features'},\n",
        "            color=sorted_importances,\n",
        "            color_continuous_scale='plasma'\n",
        "        )\n",
        "        fig.update_layout(height=max(400, len(feature_names) * 40), yaxis={'categoryorder':'total ascending'})\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"Feature importance plot error: {e}\")\n",
        "        return None\n",
        "\n",
        "# FIXED: Prediction function\n",
        "def predict_enhanced_rating(model, scaler, label_encoders, feature_names, features_dict):\n",
        "    \"\"\"Enhanced prediction with confidence intervals - COMPLETELY FIXED\"\"\"\n",
        "    if model is None:\n",
        "        return \"‚ùå Please train a model first\"\n",
        "\n",
        "    if scaler is None:\n",
        "        return \"‚ùå Model scaler not available. Please retrain the model.\"\n",
        "\n",
        "    try:\n",
        "        # Prepare input features in the same order as training\n",
        "        input_features = []\n",
        "\n",
        "        for feature in feature_names:\n",
        "            if feature == 'Reviews':\n",
        "                input_features.append(features_dict.get('reviews', 0))\n",
        "            elif feature == 'Log_Reviews':\n",
        "                input_features.append(np.log1p(features_dict.get('reviews', 0)))\n",
        "            elif feature == 'Installs':\n",
        "                input_features.append(features_dict.get('installs', 0))\n",
        "            elif feature == 'Log_Installs':\n",
        "                input_features.append(np.log1p(features_dict.get('installs', 0)))\n",
        "            elif feature == 'Size_MB':\n",
        "                input_features.append(features_dict.get('size', 25))\n",
        "            elif feature == 'Price':\n",
        "                input_features.append(features_dict.get('price', 0))\n",
        "            elif feature == 'Review_Install_Ratio':\n",
        "                reviews = features_dict.get('reviews', 0)\n",
        "                installs = features_dict.get('installs', 0)\n",
        "                ratio = reviews / (installs + 1) if installs > 0 else 0\n",
        "                input_features.append(ratio)\n",
        "            elif feature == 'Category' and label_encoders and 'Category' in label_encoders:\n",
        "                category = features_dict.get('category', 'GAME')\n",
        "                try:\n",
        "                    category_encoded = label_encoders['Category'].transform([category])[0]\n",
        "                except (ValueError, KeyError):\n",
        "                    category_encoded = 0  # Default encoding for unknown categories\n",
        "                input_features.append(category_encoded)\n",
        "            elif feature == 'Category_Popularity':\n",
        "                # Approximate category popularity\n",
        "                input_features.append(1000)  # Default moderate popularity\n",
        "\n",
        "        if not input_features:\n",
        "            return \"‚ùå No valid features provided\"\n",
        "\n",
        "        # Scale and predict\n",
        "        input_array = np.array(input_features).reshape(1, -1)\n",
        "\n",
        "        # Check if input array matches expected features\n",
        "        if input_array.shape[1] != len(feature_names):\n",
        "            return f\"‚ùå Feature mismatch. Expected {len(feature_names)} features, got {input_array.shape[1]}\"\n",
        "\n",
        "        input_scaled = scaler.transform(input_array)\n",
        "        prediction = model.predict(input_scaled)[0]\n",
        "\n",
        "        # Ensure prediction is within valid range\n",
        "        prediction = max(1.0, min(5.0, prediction))\n",
        "\n",
        "        # Get confidence estimate (for tree-based models)\n",
        "        confidence_info = \"\"\n",
        "        if hasattr(model, 'estimators_'):\n",
        "            try:\n",
        "                # For ensemble methods, get prediction variance\n",
        "                predictions = [tree.predict(input_scaled)[0] for tree in model.estimators_[:10]]  # Sample first 10 trees\n",
        "                std_pred = np.std(predictions)\n",
        "                confidence_info = f\" (¬±{std_pred:.2f} std)\"\n",
        "            except:\n",
        "                confidence_info = \"\"\n",
        "\n",
        "        # Provide interpretation\n",
        "        if prediction >= 4.5:\n",
        "            rating_interpretation = \"üåü Excellent app potential!\"\n",
        "        elif prediction >= 4.0:\n",
        "            rating_interpretation = \"üëç Good app potential!\"\n",
        "        elif prediction >= 3.5:\n",
        "            rating_interpretation = \"üòê Average app potential\"\n",
        "        else:\n",
        "            rating_interpretation = \"‚ö†Ô∏è Below average potential\"\n",
        "\n",
        "        return f\"\"\"üéØ **Predicted Rating: {prediction:.2f}‚≠ê{confidence_info}**\n",
        "\n",
        "{rating_interpretation}\n",
        "\n",
        "üìä **Input Summary:**\n",
        "‚Ä¢ Reviews: {features_dict.get('reviews', 0):,}\n",
        "‚Ä¢ Installs: {features_dict.get('installs', 0):,}\n",
        "‚Ä¢ Size: {features_dict.get('size', 25):.1f} MB\n",
        "‚Ä¢ Price: ${features_dict.get('price', 0):.2f}\n",
        "‚Ä¢ Category: {features_dict.get('category', 'GAME')}\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error making prediction: {str(e)}\"\n",
        "\n",
        "# FIXED: Enhanced filtering and insights\n",
        "def filter_apps_enhanced(df, categories, min_rating, min_installs=0, max_price=None):\n",
        "    \"\"\"Enhanced app filtering with multiple criteria - COMPLETELY FIXED\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(), \"‚ùå No data available. Please upload and process the apps data first.\"\n",
        "\n",
        "    try:\n",
        "        filtered = df.copy()\n",
        "        applied_filters = []\n",
        "\n",
        "        # FIXED: Category filter handling\n",
        "        if categories:\n",
        "            # Handle different input types\n",
        "            if isinstance(categories, str):\n",
        "                categories = [categories]\n",
        "\n",
        "            # Remove empty/None values\n",
        "            categories = [cat for cat in categories if cat and str(cat).strip() != \"\"]\n",
        "\n",
        "            if categories and \"All\" not in categories and 'Category' in df.columns:\n",
        "                # Apply category filter\n",
        "                filtered = filtered[filtered['Category'].isin(categories)]\n",
        "                applied_filters.append(f\"Categories: {', '.join(categories)}\")\n",
        "\n",
        "        # Apply rating filter\n",
        "        if 'Rating' in filtered.columns:\n",
        "            filtered = filtered[filtered['Rating'] >= min_rating]\n",
        "            applied_filters.append(f\"Min Rating: {min_rating}\")\n",
        "\n",
        "        # Apply installs filter\n",
        "        if min_installs > 0 and 'Installs' in filtered.columns:\n",
        "            filtered = filtered[filtered['Installs'] >= min_installs]\n",
        "            applied_filters.append(f\"Min Installs: {min_installs:,}\")\n",
        "\n",
        "        # Apply price filter\n",
        "        if max_price is not None and 'Price' in filtered.columns:\n",
        "            filtered = filtered[filtered['Price'] <= max_price]\n",
        "            applied_filters.append(f\"Max Price: ${max_price}\")\n",
        "\n",
        "        # Get top apps with enhanced ranking\n",
        "        if len(filtered) > 0:\n",
        "            display_cols = ['App', 'Category', 'Rating', 'Reviews', 'Installs', 'Price', 'Size_MB']\n",
        "            available_cols = [col for col in display_cols if col in filtered.columns]\n",
        "\n",
        "            # Enhanced ranking algorithm\n",
        "            if 'Installs' in filtered.columns and 'Rating' in filtered.columns:\n",
        "                # Combine rating and popularity for better ranking\n",
        "                filtered['Ranking_Score'] = (\n",
        "                    filtered['Rating'] * 0.4 +  # 40% weight to rating\n",
        "                    np.log1p(filtered['Installs']) * 0.4 +  # 40% to install popularity\n",
        "                    np.log1p(filtered['Reviews']) * 0.2  # 20% to review count\n",
        "                )\n",
        "                top_apps = filtered.nlargest(25, 'Ranking_Score')[available_cols]\n",
        "            elif 'Installs' in filtered.columns:\n",
        "                top_apps = filtered.nlargest(25, 'Installs')[available_cols]\n",
        "            elif 'Rating' in filtered.columns:\n",
        "                top_apps = filtered.nlargest(25, 'Rating')[available_cols]\n",
        "            else:\n",
        "                top_apps = filtered[available_cols].head(25)\n",
        "\n",
        "            # Clean up the display\n",
        "            if 'Ranking_Score' in top_apps.columns:\n",
        "                top_apps = top_apps.drop('Ranking_Score', axis=1)\n",
        "        else:\n",
        "            top_apps = pd.DataFrame()\n",
        "\n",
        "        # Generate enhanced insights\n",
        "        if len(filtered) > 0:\n",
        "            insights = generate_enhanced_insights(filtered, applied_filters)\n",
        "        else:\n",
        "            insights = f\"\"\"‚ùå **No apps found matching the criteria:**\n",
        "\n",
        "üîç **Applied Filters:**\n",
        "{chr(10).join([f'‚Ä¢ {f}' for f in applied_filters]) if applied_filters else '‚Ä¢ None'}\n",
        "\n",
        "üí° **Try adjusting your filters:**\n",
        "‚Ä¢ Lower the minimum rating\n",
        "‚Ä¢ Reduce minimum installs requirement\n",
        "‚Ä¢ Increase maximum price\n",
        "‚Ä¢ Select different categories\"\"\"\n",
        "\n",
        "        return top_apps, insights\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"‚ùå Error filtering data: {str(e)}\"\n",
        "\n",
        "def generate_enhanced_insights(filtered_df, applied_filters):\n",
        "    \"\"\"Generate comprehensive insights for filtered data\"\"\"\n",
        "    try:\n",
        "        total_apps = len(filtered_df)\n",
        "\n",
        "        # Basic stats\n",
        "        avg_rating = filtered_df['Rating'].mean() if 'Rating' in filtered_df.columns else 0\n",
        "        rating_std = filtered_df['Rating'].std() if 'Rating' in filtered_df.columns else 0\n",
        "\n",
        "        # Category analysis\n",
        "        top_category = \"N/A\"\n",
        "        category_diversity = 0\n",
        "        if 'Category' in filtered_df.columns and len(filtered_df) > 0:\n",
        "            category_counts = filtered_df['Category'].value_counts()\n",
        "            top_category = category_counts.index[0] if len(category_counts) > 0 else \"N/A\"\n",
        "            category_diversity = filtered_df['Category'].nunique()\n",
        "\n",
        "        # App analysis\n",
        "        most_reviewed = \"N/A\"\n",
        "        most_installed = \"N/A\"\n",
        "        highest_rated = \"N/A\"\n",
        "\n",
        "        if 'Reviews' in filtered_df.columns and len(filtered_df) > 0 and filtered_df['Reviews'].max() > 0:\n",
        "            most_reviewed = filtered_df.loc[filtered_df['Reviews'].idxmax(), 'App']\n",
        "\n",
        "        if 'Installs' in filtered_df.columns and len(filtered_df) > 0 and filtered_df['Installs'].max() > 0:\n",
        "            most_installed = filtered_df.loc[filtered_df['Installs'].idxmax(), 'App']\n",
        "\n",
        "        if 'Rating' in filtered_df.columns and len(filtered_df) > 0:\n",
        "            max_rating = filtered_df['Rating'].max()\n",
        "            highest_rated_apps = filtered_df[filtered_df['Rating'] == max_rating]['App'].head(3).tolist()\n",
        "            highest_rated = ', '.join(highest_rated_apps) if highest_rated_apps else \"N/A\"\n",
        "\n",
        "        # Price analysis\n",
        "        free_apps = paid_apps = avg_price = \"N/A\"\n",
        "        if 'Price' in filtered_df.columns:\n",
        "            free_apps = f\"{(filtered_df['Price'] == 0).sum():,}\"\n",
        "            paid_apps = f\"{(filtered_df['Price'] > 0).sum():,}\"\n",
        "            paid_df = filtered_df[filtered_df['Price'] > 0]\n",
        "            if len(paid_df) > 0:\n",
        "                avg_price = f\"${paid_df['Price'].mean():.2f}\"\n",
        "\n",
        "        # Size analysis\n",
        "        avg_size = \"N/A\"\n",
        "        if 'Size_MB' in filtered_df.columns:\n",
        "            avg_size = f\"{filtered_df['Size_MB'].mean():.1f} MB\"\n",
        "\n",
        "        insights = f\"\"\"üéØ **Enhanced Filtered Results:**\n",
        "\n",
        "üìä **Overview:**\n",
        "‚Ä¢ Apps found: **{total_apps:,}**\n",
        "‚Ä¢ Category diversity: **{category_diversity}** categories\n",
        "‚Ä¢ Average rating: **{avg_rating:.2f}** (¬±{rating_std:.2f})\n",
        "‚Ä¢ Average size: **{avg_size}**\n",
        "\n",
        "üí∞ **Pricing:**\n",
        "‚Ä¢ Free apps: **{free_apps}**\n",
        "‚Ä¢ Paid apps: **{paid_apps}**\n",
        "‚Ä¢ Avg price (paid): **{avg_price}**\n",
        "\n",
        "üèÜ **Top Performers:**\n",
        "‚Ä¢ Most reviewed: **{most_reviewed}**\n",
        "‚Ä¢ Most installed: **{most_installed}**\n",
        "‚Ä¢ Highest rated: **{highest_rated}**\n",
        "‚Ä¢ Dominant category: **{top_category}**\n",
        "\n",
        "üîç **Applied Filters:**\n",
        "{chr(10).join([f'‚Ä¢ {f}' for f in applied_filters]) if applied_filters else '‚Ä¢ None (showing all data)'}\n",
        "\n",
        "üí° **Market Insights:**\n",
        "{\"‚Ä¢ üî• Highly competitive segment!\" if total_apps > 1000 else \"‚Ä¢ üìà Moderate competition\" if total_apps > 100 else \"‚Ä¢ üéØ Low competition - great opportunity!\"}\n",
        "{\"‚Ä¢ ‚≠ê Premium quality apps!\" if avg_rating != \"N/A\" and float(avg_rating) > 4.2 else \"üëç Good quality apps\" if avg_rating != \"N/A\" and float(avg_rating) > 3.5 else \"‚ö†Ô∏è Quality improvement needed\"}\n",
        "{market_maturity_insight}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "        return insights\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating insights: {str(e)}\"\n",
        "\n",
        "# FIXED: Main Gradio Interface\n",
        "def main_interface():\n",
        "    with gr.Blocks(title=\"Enhanced Google Play Store Apps Analytics Dashboard\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üì± Enhanced Google Play Store Apps Analytics Dashboard\n",
        "\n",
        "        **üöÄ Advanced insights with machine learning predictions and comprehensive analytics**\n",
        "\n",
        "        Upload your dataset and discover deep patterns in app categories, ratings, installations, and more!\n",
        "        \"\"\")\n",
        "\n",
        "        # Global state variables\n",
        "        processed_df = gr.State()\n",
        "        processed_reviews_df = gr.State()\n",
        "        trained_model = gr.State()\n",
        "        model_scaler = gr.State()\n",
        "        model_label_encoders = gr.State()\n",
        "        model_features_state = gr.State([])\n",
        "\n",
        "        with gr.Tab(\"üìÇ Upload & Clean\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    apps_file = gr.File(\n",
        "                        label=\"Upload Apps CSV (googleplaystore.csv)\",\n",
        "                        file_types=[\".csv\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "                    upload_btn = gr.Button(\"üì§ Upload Data\", variant=\"primary\")\n",
        "                    clean_btn = gr.Button(\"üßπ Clean & Enhance Data\", variant=\"secondary\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    reviews_file = gr.File(\n",
        "                        label=\"Upload Reviews CSV (optional)\",\n",
        "                        file_types=[\".csv\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "                    process_reviews_btn = gr.Button(\"üîÑ Process Reviews Data\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    data_info = gr.Textbox(\n",
        "                        label=\"Dataset Information\",\n",
        "                        lines=15,\n",
        "                        interactive=False,\n",
        "                        value=\"Upload your CSV file to get started...\"\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    cleaning_log = gr.Textbox(\n",
        "                        label=\"Data Cleaning & Enhancement Log\",\n",
        "                        lines=15,\n",
        "                        interactive=False,\n",
        "                        value=\"Upload and clean data to see detailed processing log...\"\n",
        "                    )\n",
        "\n",
        "            data_preview = gr.Dataframe(\n",
        "                label=\"Data Preview (Enhanced)\",\n",
        "                interactive=False,\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"üìà Category Analysis\"):\n",
        "            analyze_category_btn = gr.Button(\"üîç Analyze Categories\", variant=\"primary\", size=\"lg\")\n",
        "            category_plot = gr.Plot(label=\"Category Distribution\")\n",
        "            category_metrics_table = gr.Dataframe(label=\"Comprehensive Category Metrics\")\n",
        "\n",
        "        with gr.Tab(\"‚≠ê Rating Analysis\"):\n",
        "            analyze_rating_btn = gr.Button(\"üîç Analyze Ratings\", variant=\"primary\", size=\"lg\")\n",
        "            rating_plot = gr.Plot(label=\"Rating Distribution by Install Categories\")\n",
        "            correlation_plot = gr.Plot(label=\"Enhanced Feature Correlations\")\n",
        "\n",
        "        with gr.Tab(\"üì± Install vs Reviews\"):\n",
        "            analyze_scatter_btn = gr.Button(\"üîç Analyze Relationships\", variant=\"primary\", size=\"lg\")\n",
        "            scatter_plot = gr.Plot(label=\"Advanced Installs vs Reviews Analysis\")\n",
        "\n",
        "        with gr.Tab(\"üí∞ Price Analysis\"):\n",
        "            analyze_price_btn = gr.Button(\"üîç Analyze Pricing\", variant=\"primary\", size=\"lg\")\n",
        "            price_plot = gr.Plot(label=\"Comprehensive Price Analysis\")\n",
        "\n",
        "        with gr.Tab(\"üì¶ Size & Installs\"):\n",
        "            analyze_size_btn = gr.Button(\"üîç Analyze Size & Installs\", variant=\"primary\", size=\"lg\")\n",
        "            size_plot = gr.Plot(label=\"App Size Distribution with Statistics\")\n",
        "            installs_pie_plot = gr.Plot(label=\"Enhanced Installs Category Distribution\")\n",
        "\n",
        "        with gr.Tab(\"ü©∫ Data Quality\"):\n",
        "            analyze_quality_btn = gr.Button(\"üîç Analyze Data Quality\", variant=\"primary\", size=\"lg\")\n",
        "            missing_plot = gr.Plot(label=\"Missing Values Analysis\")\n",
        "            duplicates_info = gr.Markdown()\n",
        "\n",
        "        with gr.Tab(\"üòä Sentiment Analysis\"):\n",
        "            analyze_sentiment_btn = gr.Button(\"üîç Analyze Sentiments\", variant=\"primary\", size=\"lg\")\n",
        "            sentiment_plot = gr.Plot(label=\"User Review Sentiments\")\n",
        "            sentiment_info = gr.Textbox(\n",
        "                label=\"Sentiment Analysis Info\",\n",
        "                lines=5,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"ü§ñ Enhanced ML Prediction\"):\n",
        "            gr.Markdown(\"### üöÄ Advanced Rating Prediction with Multiple Models\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_features = gr.CheckboxGroup(\n",
        "                        label=\"Select Features for Enhanced Prediction\",\n",
        "                        choices=[\"Reviews\", \"Log_Reviews\", \"Installs\", \"Log_Installs\", \"Size_MB\", \"Price\", \"Category\", \"Review_Install_Ratio\", \"Category_Popularity\"],\n",
        "                        value=[\"Log_Reviews\", \"Log_Installs\", \"Review_Install_Ratio\"],\n",
        "                        interactive=True\n",
        "                    )\n",
        "                    train_model_btn = gr.Button(\"üîÆ Train Enhanced Models\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    model_results = gr.Textbox(\n",
        "                        label=\"Enhanced Model Performance Results\",\n",
        "                        lines=12,\n",
        "                        interactive=False,\n",
        "                        placeholder=\"Train models to see comprehensive performance comparison...\"\n",
        "                    )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    predict_reviews = gr.Number(label=\"Reviews Count\", value=5000, precision=0)\n",
        "                    predict_installs = gr.Number(label=\"Installs Count\", value=100000, precision=0)\n",
        "                    predict_size = gr.Number(label=\"Size (MB)\", value=25.0)\n",
        "                    predict_price = gr.Number(label=\"Price ($)\", value=0.0)\n",
        "\n",
        "                with gr.Column():\n",
        "                    predict_category = gr.Dropdown(\n",
        "                        label=\"App Category\",\n",
        "                        choices=[\"GAME\", \"COMMUNICATION\", \"TOOLS\", \"PRODUCTIVITY\", \"ENTERTAINMENT\", \"SOCIAL\", \"EDUCATION\", \"PHOTOGRAPHY\", \"SHOPPING\", \"TRAVEL_AND_LOCAL\"],\n",
        "                        value=\"GAME\"\n",
        "                    )\n",
        "                    predict_btn = gr.Button(\"üéØ Predict Rating\", variant=\"secondary\", size=\"lg\")\n",
        "                    predicted_rating = gr.Textbox(\n",
        "                        label=\"Enhanced Prediction Result\",\n",
        "                        interactive=False,\n",
        "                        lines=8,\n",
        "                        placeholder=\"Train model and enter values to get detailed prediction...\"\n",
        "                    )\n",
        "\n",
        "            feature_importance_plot = gr.Plot(label=\"Enhanced Feature Importance Analysis\")\n",
        "\n",
        "        with gr.Tab(\"üéØ Enhanced App Insights\"):\n",
        "            gr.Markdown(\"### üîç Advanced Multi-Criteria App Filtering\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    category_filter = gr.Dropdown(\n",
        "                        label=\"Select Categories (Multi-select)\",\n",
        "                        choices=[\"All\"],\n",
        "                        value=[\"All\"],\n",
        "                        multiselect=True\n",
        "                    )\n",
        "                    rating_filter = gr.Slider(\n",
        "                        minimum=1.0,\n",
        "                        maximum=5.0,\n",
        "                        value=4.0,\n",
        "                        step=0.1,\n",
        "                        label=\"Minimum Rating\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    installs_filter = gr.Number(\n",
        "                        label=\"Minimum Installs\",\n",
        "                        value=0,\n",
        "                        precision=0\n",
        "                    )\n",
        "                    price_filter = gr.Number(\n",
        "                        label=\"Maximum Price ($)\",\n",
        "                        value=100,\n",
        "                        precision=2\n",
        "                    )\n",
        "\n",
        "            filter_btn = gr.Button(\"üîç Apply Enhanced Filters\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            filtered_apps = gr.Dataframe(\n",
        "                label=\"Top Filtered Apps (Enhanced Ranking)\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            insights_text = gr.Textbox(\n",
        "                label=\"Comprehensive Insights & Analytics\",\n",
        "                lines=12,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        # FIXED: Event handler functions\n",
        "        def upload_data(file_path):\n",
        "            if file_path is None:\n",
        "                return \"‚ùå No file uploaded\", pd.DataFrame(), None, gr.Dropdown(choices=[\"All\"], value=[\"All\"]), []\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                if df.empty:\n",
        "                    return \"‚ùå CSV file is empty\", pd.DataFrame(), None, gr.Dropdown(choices=[\"All\"], value=[\"All\"]), []\n",
        "\n",
        "                info = f\"\"\"üì§ **Data Upload Successful!**\n",
        "‚Ä¢ File loaded: {len(df):,} rows, {len(df.columns)} columns\n",
        "‚Ä¢ Columns: {', '.join(df.columns.tolist())}\n",
        "‚Ä¢ Memory usage: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\n",
        "\n",
        "üîÑ **Next Step:** Click \"Clean & Enhance Data\" to apply advanced preprocessing and feature engineering\"\"\"\n",
        "\n",
        "                preview_df = df.head(10)\n",
        "                categories = get_category_choices(df)\n",
        "\n",
        "                return info, preview_df, df, gr.Dropdown(choices=categories, value=[\"All\"]), []\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Error loading file: {str(e)}\", pd.DataFrame(), None, gr.Dropdown(choices=[\"All\"], value=[\"All\"]), []\n",
        "\n",
        "        def clean_data(df):\n",
        "            if df is None or df.empty:\n",
        "                return \"‚ùå No data to clean. Please upload a file first.\", pd.DataFrame(), None, gr.Dropdown(choices=[\"All\"], value=[\"All\"]), \"‚ùå No cleaning performed\"\n",
        "\n",
        "            try:\n",
        "                df_processed, log = preprocess_data(df.copy())\n",
        "\n",
        "                info = get_safe_info_string(df_processed)\n",
        "                preview_df = get_safe_preview(df_processed)\n",
        "                categories = get_category_choices(df_processed)\n",
        "                log_text = \"\\n\".join(log)\n",
        "\n",
        "                return info, preview_df, df_processed, gr.Dropdown(choices=categories, value=[\"All\"]), log_text\n",
        "\n",
        "            except Exception as e:\n",
        "                error_log = f\"‚ùå Error during cleaning: {str(e)}\"\n",
        "                return f\"‚ùå Error cleaning data: {str(e)}\", pd.DataFrame(), None, gr.Dropdown(choices=[\"All\"], value=[\"All\"]), error_log\n",
        "\n",
        "        def process_reviews_data(file_path):\n",
        "            if file_path is None:\n",
        "                return \"‚ùå No reviews file uploaded\", None\n",
        "\n",
        "            try:\n",
        "                df_reviews = pd.read_csv(file_path)\n",
        "                df_reviews_processed = preprocess_reviews_data(df_reviews.copy())\n",
        "\n",
        "                unique_apps = df_reviews_processed['App'].nunique() if 'App' in df_reviews_processed.columns else 0\n",
        "\n",
        "                info = f\"\"\"üìù **Enhanced Reviews Dataset Overview:**\n",
        "‚Ä¢ Total Reviews: {len(df_reviews_processed):,}\n",
        "‚Ä¢ Unique Apps: {unique_apps:,}\n",
        "‚Ä¢ Columns: {', '.join(df_reviews_processed.columns.tolist())}\n",
        "\n",
        "‚úÖ **Reviews data successfully processed!**\"\"\"\n",
        "\n",
        "                return info, df_reviews_processed\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Error processing reviews file: {str(e)}\", None\n",
        "\n",
        "        def filter_apps_handler(df, categories, min_rating, min_installs, max_price):\n",
        "            \"\"\"Enhanced filtering handler - FIXED\"\"\"\n",
        "            if df is None or df.empty:\n",
        "                return pd.DataFrame(), \"‚ùå No data available. Please upload and process the apps data first.\"\n",
        "\n",
        "            return filter_apps_enhanced(df, categories, min_rating, min_installs, max_price)\n",
        "\n",
        "        # Individual analysis functions\n",
        "        def analyze_categories(df):\n",
        "            if df is None or df.empty:\n",
        "                return None, pd.DataFrame()\n",
        "            category_fig = create_category_distribution(df)\n",
        "            metrics_table = create_category_metrics(df)\n",
        "            return category_fig, metrics_table\n",
        "\n",
        "        def analyze_ratings(df):\n",
        "            if df is None or df.empty:\n",
        "                return None, None\n",
        "            rating_fig = create_rating_distribution(df)\n",
        "            corr_fig = create_correlation_heatmap(df)\n",
        "            return rating_fig, corr_fig\n",
        "\n",
        "        def analyze_scatter(df):\n",
        "            if df is None or df.empty:\n",
        "                return None\n",
        "            return create_installs_vs_reviews(df)\n",
        "\n",
        "        def analyze_pricing(df):\n",
        "            if df is None or df.empty:\n",
        "                return None\n",
        "            return create_price_analysis(df)\n",
        "\n",
        "        def analyze_size_installs(df):\n",
        "            if df is None or df.empty:\n",
        "                return None, None\n",
        "            size_fig = create_size_distribution(df)\n",
        "            installs_fig = create_installs_pie(df)\n",
        "            return size_fig, installs_fig\n",
        "\n",
        "        def analyze_data_quality(df):\n",
        "            if df is None or df.empty:\n",
        "                return None, \"No data available\"\n",
        "            missing_fig = create_missing_values_plot(df)\n",
        "            dup_info = get_duplicates_info(df)\n",
        "            return missing_fig, dup_info\n",
        "\n",
        "        def analyze_sentiments(df_reviews):\n",
        "            if df_reviews is None or df_reviews.empty:\n",
        "                return None, \"‚ùå No reviews data available. Please upload the reviews CSV file first.\"\n",
        "\n",
        "            sentiment_fig = create_sentiment_analysis(df_reviews)\n",
        "\n",
        "            if sentiment_fig is None:\n",
        "                info = \"‚ùå Sentiment column not found in reviews data. Please ensure your reviews CSV has a 'Sentiment' column.\"\n",
        "            else:\n",
        "                sentiment_stats = df_reviews['Sentiment'].value_counts()\n",
        "                info = f\"\"\"üìä **Enhanced Sentiment Analysis Results:**\n",
        "‚Ä¢ Total Reviews Analyzed: {len(df_reviews):,}\n",
        "‚Ä¢ Positive Reviews: {sentiment_stats.get('Positive', 0):,} ({sentiment_stats.get('Positive', 0)/len(df_reviews)*100:.1f}%)\n",
        "‚Ä¢ Negative Reviews: {sentiment_stats.get('Negative', 0):,} ({sentiment_stats.get('Negative', 0)/len(df_reviews)*100:.1f}%)\n",
        "‚Ä¢ Neutral Reviews: {sentiment_stats.get('Neutral', 0):,} ({sentiment_stats.get('Neutral', 0)/len(df_reviews)*100:.1f}%)\n",
        "\n",
        "‚úÖ **Enhanced analysis complete!**\"\"\"\n",
        "\n",
        "            return sentiment_fig, info\n",
        "\n",
        "        def train_model_handler(df, selected_features):\n",
        "            \"\"\"Handle enhanced model training - FIXED\"\"\"\n",
        "            if df is None or df.empty:\n",
        "                return \"‚ùå No data available. Please upload and clean data first.\", None, None, None, None, []\n",
        "\n",
        "            if not selected_features:\n",
        "                return \"‚ùå Please select at least one feature for training.\", None, None, None, None, []\n",
        "\n",
        "            results, model, scaler, importance_fig, label_encoders = train_enhanced_rating_prediction_model(df, selected_features)\n",
        "            return results, model, scaler, importance_fig, label_encoders, selected_features\n",
        "\n",
        "        def predict_rating_handler(model, scaler, label_encoders, model_features, reviews, installs, size, price, category):\n",
        "            \"\"\"Handle enhanced rating prediction - FIXED\"\"\"\n",
        "            if model is None:\n",
        "                return \"‚ùå Please train a model first by selecting features and clicking 'Train Enhanced Models'.\"\n",
        "\n",
        "            if not model_features:\n",
        "                return \"‚ùå No model features available. Please train the model first.\"\n",
        "\n",
        "            features_dict = {\n",
        "                'reviews': reviews,\n",
        "                'installs': installs,\n",
        "                'size': size,\n",
        "                'price': price,\n",
        "                'category': category\n",
        "            }\n",
        "\n",
        "            return predict_enhanced_rating(model, scaler, label_encoders, model_features, features_dict)\n",
        "\n",
        "        # FIXED: Event handlers\n",
        "        upload_btn.click(\n",
        "            fn=upload_data,\n",
        "            inputs=[apps_file],\n",
        "            outputs=[data_info, data_preview, processed_df, category_filter, cleaning_log]\n",
        "        )\n",
        "\n",
        "        clean_btn.click(\n",
        "            fn=clean_data,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[data_info, data_preview, processed_df, category_filter, cleaning_log]\n",
        "        )\n",
        "\n",
        "        process_reviews_btn.click(\n",
        "            fn=process_reviews_data,\n",
        "            inputs=[reviews_file],\n",
        "            outputs=[data_info, processed_reviews_df]\n",
        "        )\n",
        "\n",
        "        # Analysis Event Handlers\n",
        "        analyze_category_btn.click(\n",
        "            fn=analyze_categories,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[category_plot, category_metrics_table]\n",
        "        )\n",
        "\n",
        "        analyze_rating_btn.click(\n",
        "            fn=analyze_ratings,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[rating_plot, correlation_plot]\n",
        "        )\n",
        "\n",
        "        analyze_scatter_btn.click(\n",
        "            fn=analyze_scatter,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[scatter_plot]\n",
        "        )\n",
        "\n",
        "        analyze_price_btn.click(\n",
        "            fn=analyze_pricing,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[price_plot]\n",
        "        )\n",
        "\n",
        "        analyze_size_btn.click(\n",
        "            fn=analyze_size_installs,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[size_plot, installs_pie_plot]\n",
        "        )\n",
        "\n",
        "        analyze_quality_btn.click(\n",
        "            fn=analyze_data_quality,\n",
        "            inputs=[processed_df],\n",
        "            outputs=[missing_plot, duplicates_info]\n",
        "        )\n",
        "\n",
        "        analyze_sentiment_btn.click(\n",
        "            fn=analyze_sentiments,\n",
        "            inputs=[processed_reviews_df],\n",
        "            outputs=[sentiment_plot, sentiment_info]\n",
        "        )\n",
        "\n",
        "        # Machine Learning Event Handlers\n",
        "        train_model_btn.click(\n",
        "            fn=train_model_handler,\n",
        "            inputs=[processed_df, model_features],\n",
        "            outputs=[model_results, trained_model, model_scaler, feature_importance_plot,\n",
        "                    model_label_encoders, model_features_state]\n",
        "        )\n",
        "\n",
        "        predict_btn.click(\n",
        "            fn=predict_rating_handler,\n",
        "            inputs=[trained_model, model_scaler, model_label_encoders, model_features_state,\n",
        "                   predict_reviews, predict_installs, predict_size, predict_price, predict_category],\n",
        "            outputs=[predicted_rating]\n",
        "        )\n",
        "\n",
        "        # FIXED: App Insights Event Handler\n",
        "        filter_btn.click(\n",
        "            fn=filter_apps_handler,\n",
        "            inputs=[processed_df, category_filter, rating_filter, installs_filter, price_filter],\n",
        "            outputs=[filtered_apps, insights_text]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Gradio interface\"\"\"\n",
        "    try:\n",
        "        demo = main_interface()\n",
        "        demo.launch(\n",
        "            share=True,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=7860,\n",
        "            show_error=True,\n",
        "            debug=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error launching interface: {e}\")\n",
        "        # Fallback to basic launch\n",
        "        demo = main_interface()\n",
        "        demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}